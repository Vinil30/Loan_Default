{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82312554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LoanID', 'Age', 'Income', 'LoanAmount', 'CreditScore',\n",
      "       'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm',\n",
      "       'DTIRatio', 'Education', 'EmploymentType', 'MaritalStatus',\n",
      "       'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner',\n",
      "       'Default'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"Loan_default.csv\")\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7764824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "\n",
      "First 5 rows:\n",
      "       LoanID  Age  Income  LoanAmount  CreditScore  MonthsEmployed  \\\n",
      "0  I38PQUQS96   56   85994       50587          520              80   \n",
      "1  HPSK72WA7R   69   50432      124440          458              15   \n",
      "2  C1OZ6DPJ8Y   46   84208      129188          451              26   \n",
      "3  V2KKSFM3UN   32   31713       44799          743               0   \n",
      "4  EY08JDHTZP   60   20437        9139          633               8   \n",
      "\n",
      "   NumCreditLines  InterestRate  LoanTerm  DTIRatio    Education  \\\n",
      "0               4         15.23        36      0.44   Bachelor's   \n",
      "1               1          4.81        60      0.68     Master's   \n",
      "2               3         21.17        24      0.31     Master's   \n",
      "3               3          7.07        24      0.23  High School   \n",
      "4               4          6.51        48      0.73   Bachelor's   \n",
      "\n",
      "  EmploymentType MaritalStatus HasMortgage HasDependents LoanPurpose  \\\n",
      "0      Full-time      Divorced         Yes           Yes       Other   \n",
      "1      Full-time       Married          No            No       Other   \n",
      "2     Unemployed      Divorced         Yes           Yes        Auto   \n",
      "3      Full-time       Married          No            No    Business   \n",
      "4     Unemployed      Divorced          No           Yes        Auto   \n",
      "\n",
      "  HasCoSigner  Default  \n",
      "0         Yes        0  \n",
      "1         Yes        0  \n",
      "2          No        1  \n",
      "3          No        0  \n",
      "4          No        0  \n",
      "\n",
      "Column names in the dataset:\n",
      "['LoanID', 'Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner', 'Default']\n",
      "\n",
      "Features shape: (255347, 16)\n",
      "Target distribution:\n",
      "Default\n",
      "0    225694\n",
      "1     29653\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric features (9): ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n",
      "Categorical features (7): ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\n",
      "\n",
      "Preprocessor (scaler + encoder) saved as preprocessor.pkl\n",
      "\n",
      "Class weights: {np.int64(0): np.float64(0.5656918944365983), np.int64(1): np.float64(4.30564454936346)}\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VINIL\\Desktop\\TechNova\\Environment\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6715 - loss: 0.6085 - val_accuracy: 0.7049 - val_loss: 0.5675\n",
      "Epoch 2/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6845 - loss: 0.5906 - val_accuracy: 0.7058 - val_loss: 0.5629\n",
      "Epoch 3/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6859 - loss: 0.5892 - val_accuracy: 0.6887 - val_loss: 0.5891\n",
      "Epoch 4/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6857 - loss: 0.5887 - val_accuracy: 0.6896 - val_loss: 0.5871\n",
      "Epoch 5/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6881 - loss: 0.5886 - val_accuracy: 0.6627 - val_loss: 0.6210\n",
      "Epoch 6/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6889 - loss: 0.5883 - val_accuracy: 0.6889 - val_loss: 0.5848\n",
      "Epoch 7/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6873 - loss: 0.5882 - val_accuracy: 0.7139 - val_loss: 0.5606\n",
      "Epoch 8/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6885 - loss: 0.5880 - val_accuracy: 0.6438 - val_loss: 0.6326\n",
      "Epoch 9/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6877 - loss: 0.5880 - val_accuracy: 0.6609 - val_loss: 0.6199\n",
      "Epoch 10/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.6886 - loss: 0.5876 - val_accuracy: 0.6860 - val_loss: 0.5950\n",
      "Epoch 11/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.6880 - loss: 0.5877 - val_accuracy: 0.7169 - val_loss: 0.5553\n",
      "Epoch 12/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6887 - loss: 0.5877 - val_accuracy: 0.6843 - val_loss: 0.5917\n",
      "Epoch 13/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6883 - loss: 0.5875 - val_accuracy: 0.6241 - val_loss: 0.6556\n",
      "Epoch 14/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6867 - loss: 0.5872 - val_accuracy: 0.6993 - val_loss: 0.5732\n",
      "Epoch 15/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6884 - loss: 0.5873 - val_accuracy: 0.7059 - val_loss: 0.5680\n",
      "Epoch 16/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6897 - loss: 0.5872 - val_accuracy: 0.6850 - val_loss: 0.5842\n",
      "Epoch 17/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.6865 - loss: 0.5872 - val_accuracy: 0.7128 - val_loss: 0.5588\n",
      "Epoch 18/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6890 - loss: 0.5870 - val_accuracy: 0.7170 - val_loss: 0.5556\n",
      "Epoch 19/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.6880 - loss: 0.5868 - val_accuracy: 0.6794 - val_loss: 0.5950\n",
      "Epoch 20/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.6889 - loss: 0.5867 - val_accuracy: 0.6856 - val_loss: 0.5914\n",
      "Epoch 21/50\n",
      "\u001b[1m5107/5107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.6890 - loss: 0.5868 - val_accuracy: 0.6746 - val_loss: 0.6008\n",
      "\u001b[1m1596/1596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490us/step\n",
      "\n",
      "✅ Test Accuracy: 0.7165\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32677 12462]\n",
      " [ 2014  3917]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.72      0.82     45139\n",
      "           1       0.24      0.66      0.35      5931\n",
      "\n",
      "    accuracy                           0.72     51070\n",
      "   macro avg       0.59      0.69      0.58     51070\n",
      "weighted avg       0.86      0.72      0.76     51070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load dataset\n",
    "# --------------------------------------------------\n",
    "file_name = 'Loan_default.csv'\n",
    "if not os.path.exists(file_name):\n",
    "    raise FileNotFoundError(f\"{file_name} not found. Please place the file in the current directory.\")\n",
    "\n",
    "dataset = pd.read_csv(file_name)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(dataset.head())\n",
    "print(\"\\nColumn names in the dataset:\")\n",
    "print(dataset.columns.tolist())\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Define features and target\n",
    "# --------------------------------------------------\n",
    "# Drop identifier column\n",
    "if 'LoanID' in dataset.columns:\n",
    "    dataset = dataset.drop(columns=['LoanID'])\n",
    "\n",
    "# Target column\n",
    "target_col = 'Default'\n",
    "if target_col not in dataset.columns:\n",
    "    raise KeyError(f\"Target column '{target_col}' not found.\")\n",
    "\n",
    "# Separate features and target\n",
    "X = dataset.drop(columns=[target_col])\n",
    "y = dataset[target_col]\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Train/test split (stratified)\n",
    "# --------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Preprocessing pipeline\n",
    "# --------------------------------------------------\n",
    "# Numeric transformer: scaling\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# Categorical transformer: one-hot encoding\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Combine into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit preprocessor on training data and transform both sets\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Save the preprocessor (includes both scaler and encoder)\n",
    "with open(\"preprocessor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "print(\"\\nPreprocessor (scaler + encoder) saved as preprocessor.pkl\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Build ANN model\n",
    "# --------------------------------------------------\n",
    "input_dim = X_train_processed.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(units=8, activation='relu', input_dim=input_dim))\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. Handle class imbalance\n",
    "# --------------------------------------------------\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, weights))\n",
    "print(\"\\nClass weights:\", class_weight_dict)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. Train with early stopping\n",
    "# --------------------------------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_processed, y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 8. Evaluate on test set\n",
    "# --------------------------------------------------\n",
    "y_pred_prob = model.predict(X_test_processed)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n✅ Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 9. Save the trained model\n",
    "# --------------------------------------------------\n",
    "# model.save(\"ann_model.h5\")\n",
    "# print(\"Model saved as ann_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11561ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as ann_model.h5\n"
     ]
    }
   ],
   "source": [
    "model.save(\"ann_model.h5\")\n",
    "print(\"Model saved as ann_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128cd52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted class: 1\n",
      "Probability of default: 0.5755\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load the saved preprocessor and model\n",
    "# --------------------------------------------------\n",
    "def load_artifacts(preprocessor_path='preprocessor.pkl', model_path='ann_model.h5'):\n",
    "    \"\"\"Load the preprocessor (scaler + encoder) and the trained Keras model.\"\"\"\n",
    "    with open(preprocessor_path, 'rb') as f:\n",
    "        preprocessor = pickle.load(f)\n",
    "    model = load_model(model_path)\n",
    "    return preprocessor, model\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Predict function for a single sample or DataFrame\n",
    "# --------------------------------------------------\n",
    "def predict_loan_default(input_data, preprocessor, model, return_prob=False):\n",
    "    \"\"\"\n",
    "    Predict loan default for new input data.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_data: pandas DataFrame or dict (will be converted to DataFrame)\n",
    "    - preprocessor: fitted ColumnTransformer\n",
    "    - model: trained Keras model\n",
    "    - return_prob: if True, return probability scores; else return binary class (0/1)\n",
    "    \n",
    "    Returns:\n",
    "    - predictions: numpy array of predictions\n",
    "    \"\"\"\n",
    "    # Ensure input is a DataFrame\n",
    "    if isinstance(input_data, dict):\n",
    "        input_data = pd.DataFrame([input_data])\n",
    "    elif not isinstance(input_data, pd.DataFrame):\n",
    "        raise ValueError(\"input_data must be a pandas DataFrame or a dict\")\n",
    "    \n",
    "    # Apply the same preprocessing (scaling + one-hot encoding)\n",
    "    X_processed = preprocessor.transform(input_data)\n",
    "    \n",
    "    # Predict\n",
    "    pred_prob = model.predict(X_processed).flatten()\n",
    "    \n",
    "    if return_prob:\n",
    "        return pred_prob\n",
    "    else:\n",
    "        return (pred_prob > 0.5).astype(int)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Example usage\n",
    "# --------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load artifacts\n",
    "    preprocessor, model = load_artifacts()\n",
    "    \n",
    "    # Example 1: Predict on a single new applicant (as a dict)\n",
    "    new_applicant = {\n",
    "        'Age': 50,\n",
    "        'Income': 34641,\n",
    "        'LoanAmount': 108855,\n",
    "        'CreditScore': 347,\n",
    "        'MonthsEmployed': 17,\n",
    "        'NumCreditLines': 4,\n",
    "        'InterestRate': 11.77,\n",
    "        'LoanTerm': 24,\n",
    "        'DTIRatio': 0.47,\n",
    "        'Education': \"PhD\",\n",
    "        'EmploymentType': 'Unemployed',\n",
    "        'MaritalStatus': 'Divorced',\n",
    "        'HasMortgage': 'Yes',\n",
    "        'HasDependents': 'No',\n",
    "        'LoanPurpose': 'Business',\n",
    "        'HasCoSigner': 'No'\n",
    "    }\n",
    "    # Q8OXUYF0TI,50,34641,108855,347,17,4,11.77,24,0.47,PhD,Unemployed,Divorced,Yes,No,Business,No,1\n",
    "\n",
    "    pred_class = predict_loan_default(new_applicant, preprocessor, model, return_prob=False)\n",
    "    pred_prob = predict_loan_default(new_applicant, preprocessor, model, return_prob=True)\n",
    "    \n",
    "    print(f\"Predicted class: {pred_class[0]}\")\n",
    "    print(f\"Probability of default: {pred_prob[0]:.4f}\")\n",
    "    \n",
    "    # Example 2: Predict on a batch from a CSV file (e.g., new_applicants.csv)\n",
    "    # new_data = pd.read_csv('new_applicants.csv')\n",
    "    # predictions = predict_loan_default(new_data, preprocessor, model)\n",
    "    # print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
